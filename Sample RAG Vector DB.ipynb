{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb04e84b-623c-4727-a4e0-d6e7cdbb266f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: mlflow==2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (2.10.1)\nRequirement already satisfied: lxml==4.9.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (4.9.3)\nRequirement already satisfied: transformers==4.30.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: langchain==0.1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (0.1.5)\nRequirement already satisfied: databricks-vectorsearch==0.22 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (0.22)\nRequirement already satisfied: gunicorn<22 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (20.1.0)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (4.11.3)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (2.0.0)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (2.2.5)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (3.7.0)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from mlflow==2.10.1) (2.0.30)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (0.18.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (0.4.2)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (2022.7)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (8.0.4)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (0.4)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from mlflow==2.10.1) (1.13.1)\nRequirement already satisfied: querystring-parser<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from mlflow==2.10.1) (1.2.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (2.28.1)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (1.10.0)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (3.4.1)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (4.24.0)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from mlflow==2.10.1) (7.0.0)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (3.1.2)\nRequirement already satisfied: packaging<24 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (23.2)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (8.0.0)\nRequirement already satisfied: pyyaml<7,>=5.1 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (6.0)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (3.1.27)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (1.5.3)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (1.23.5)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (1.1.1)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (3.9.0)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (2022.7.9)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (0.19.4)\nRequirement already satisfied: safetensors>=0.3.1 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (4.64.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from transformers==4.30.2) (0.13.3)\nRequirement already satisfied: langchain-community<0.1,>=0.0.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (0.0.20)\nRequirement already satisfied: pydantic<3,>=1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (1.10.9)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.1.5) (3.9.1)\nRequirement already satisfied: langchain-core<0.2,>=0.1.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (0.1.23)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.1.5) (1.33)\nRequirement already satisfied: langsmith<0.1,>=0.0.83 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (0.0.87)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (0.5.14)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.1.5) (4.0.3)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (8.3.0)\nRequirement already satisfied: mlflow-skinny<3,>=2.4.0 in /databricks/python3/lib/python3.10/site-packages (from databricks-vectorsearch==0.22) (2.9.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.9.4)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (6.0.4)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (22.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.3.1)\nRequirement already satisfied: typing-extensions>=4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.10.1) (4.11.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.10.1) (1.2.0)\nRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (3.2.0)\nRequirement already satisfied: urllib3<3,>=1.26.7 in /databricks/python3/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (1.26.14)\nRequirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (2.3.0)\nRequirement already satisfied: tabulate>=0.7.7 in /databricks/python3/lib/python3.10/site-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (0.8.10)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (0.9.0)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow==2.10.1) (2.2.2)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow==2.10.1) (2.0.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow==2.10.1) (4.0.11)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.10/site-packages (from gunicorn<22->mlflow==2.10.1) (65.6.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2023.6.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.10.1) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.10.1) (2.1.1)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.5) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /databricks/python3/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.5) (3.5.0)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (4.25.0)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (2.8.2)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (1.0.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (0.11.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (9.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.10.1) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.10.1) (2022.12.7)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3,>=2.17.3->mlflow==2.10.1) (2.0.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.10.1) (2.2.0)\nRequirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.10.1) (1.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.10.1) (2.0.1)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain==0.1.5) (1.2.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.10.1) (5.0.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.5) (0.4.3)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow==2.10.1 lxml==4.9.3 transformers==4.30.2 langchain==0.1.5 databricks-vectorsearch==0.22\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b90076-769a-4a91-999e-064d37e13e91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: transformers==4.30.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: unstructured[docx,pdf]==0.10.30 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (0.10.30)\nRequirement already satisfied: langchain==0.1.5 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (0.1.5)\nRequirement already satisfied: llama-index==0.9.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (0.9.3)\nRequirement already satisfied: databricks-vectorsearch==0.22 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (0.22)\nRequirement already satisfied: pydantic==1.10.9 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (1.10.9)\nRequirement already satisfied: mlflow==2.10.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (2.10.1)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (1.23.5)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (2.28.1)\nRequirement already satisfied: safetensors>=0.3.1 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (0.4.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (4.64.1)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (2022.7.9)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (3.9.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from transformers==4.30.2) (0.13.3)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (23.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /databricks/python3/lib/python3.10/site-packages (from transformers==4.30.2) (0.19.4)\nRequirement already satisfied: emoji in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (2.11.1)\nRequirement already satisfied: dataclasses-json in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (0.5.14)\nRequirement already satisfied: filetype in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (1.2.0)\nRequirement already satisfied: chardet in /databricks/python3/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (4.0.0)\nRequirement already satisfied: nltk in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (3.8.1)\nRequirement already satisfied: lxml in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (4.9.3)\nRequirement already satisfied: langdetect in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (1.0.9)\nRequirement already satisfied: backoff in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (2.2.1)\nRequirement already satisfied: beautifulsoup4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (4.12.3)\nRequirement already satisfied: typing-extensions in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (4.11.0)\nRequirement already satisfied: python-magic in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (0.4.27)\nRequirement already satisfied: python-iso639 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (2024.4.27)\nRequirement already satisfied: rapidfuzz in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (3.9.0)\nRequirement already satisfied: tabulate in /databricks/python3/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (0.8.10)\nRequirement already satisfied: python-docx>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (1.1.2)\nRequirement already satisfied: pdfminer.six in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (20231228)\nRequirement already satisfied: unstructured.pytesseract>=0.3.12 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (0.3.12)\nRequirement already satisfied: unstructured-inference==0.7.11 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (0.7.11)\nRequirement already satisfied: onnx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (1.16.0)\nRequirement already satisfied: pdf2image in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured[docx,pdf]==0.10.30) (1.17.0)\nRequirement already satisfied: langsmith<0.1,>=0.0.83 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (0.0.87)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.1.5) (4.0.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.1.5) (1.33)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (2.0.30)\nRequirement already satisfied: langchain-community<0.1,>=0.0.17 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (0.0.20)\nRequirement already satisfied: langchain-core<0.2,>=0.1.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (0.1.23)\nRequirement already satisfied: tenacity<9.0.0,>=8.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from langchain==0.1.5) (8.3.0)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /databricks/python3/lib/python3.10/site-packages (from langchain==0.1.5) (3.9.1)\nRequirement already satisfied: pandas[jinja2] in /databricks/python3/lib/python3.10/site-packages (from llama-index==0.9.3) (1.5.3)\nRequirement already satisfied: deprecated>=1.2.9.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from llama-index==0.9.3) (1.2.14)\nRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from llama-index==0.9.3) (1.6.0)\nRequirement already satisfied: httpx in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from llama-index==0.9.3) (0.27.0)\nRequirement already satisfied: urllib3<2 in /databricks/python3/lib/python3.10/site-packages (from llama-index==0.9.3) (1.26.14)\nRequirement already satisfied: typing-inspect>=0.8.0 in /databricks/python3/lib/python3.10/site-packages (from llama-index==0.9.3) (0.9.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.10/site-packages (from llama-index==0.9.3) (2023.6.0)\nRequirement already satisfied: tiktoken>=0.3.3 in /databricks/python3/lib/python3.10/site-packages (from llama-index==0.9.3) (0.5.2)\nRequirement already satisfied: aiostream<0.6.0,>=0.5.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from llama-index==0.9.3) (0.5.2)\nRequirement already satisfied: openai>=1.1.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from llama-index==0.9.3) (1.26.0)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.10/site-packages (from databricks-vectorsearch==0.22) (4.24.0)\nRequirement already satisfied: mlflow-skinny<3,>=2.4.0 in /databricks/python3/lib/python3.10/site-packages (from databricks-vectorsearch==0.22) (2.9.2)\nRequirement already satisfied: databricks-cli<1,>=0.8.7 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (0.18.0)\nRequirement already satisfied: gunicorn<22 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (20.1.0)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (0.4.2)\nRequirement already satisfied: pytz<2024 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (2022.7)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (3.1.27)\nRequirement already satisfied: docker<8,>=4.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from mlflow==2.10.1) (7.0.0)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (8.0.0)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (1.1.1)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (2.0.0)\nRequirement already satisfied: alembic!=1.10.0,<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from mlflow==2.10.1) (1.13.1)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (0.4)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (2.2.5)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (1.10.0)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (3.1.2)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (3.7.0)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (8.0.4)\nRequirement already satisfied: querystring-parser<2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from mlflow==2.10.1) (1.2.4)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (4.11.3)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow==2.10.1) (3.4.1)\nRequirement already satisfied: opencv-python!=4.7.0.68 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (4.9.0.80)\nRequirement already satisfied: layoutparser[layoutmodels,tesseract] in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (0.3.4)\nRequirement already satisfied: python-multipart in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (0.0.9)\nRequirement already satisfied: onnxruntime<1.16 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (1.15.1)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (22.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.9.4)\nRequirement already satisfied: multidict<7.0,>=4.5 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (6.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /databricks/python3/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.5) (1.3.1)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow==2.10.1) (1.2.0)\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.10/site-packages (from beautifulsoup4->unstructured[docx,pdf]==0.10.30) (2.3.2.post1)\nRequirement already satisfied: pyjwt>=1.7.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (2.3.0)\nRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (1.16.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow==2.10.1) (3.2.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /databricks/python3/lib/python3.10/site-packages (from dataclasses-json->unstructured[docx,pdf]==0.10.30) (3.20.2)\nRequirement already satisfied: wrapt<2,>=1.10 in /databricks/python3/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index==0.9.3) (1.14.1)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow==2.10.1) (2.0.1)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow==2.10.1) (2.2.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython<4,>=2.1.0->mlflow==2.10.1) (4.0.11)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.10/site-packages (from gunicorn<22->mlflow==2.10.1) (65.6.3)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow==2.10.1) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow==2.10.1) (2.1.1)\nRequirement already satisfied: jsonpointer>=1.9 in /databricks/python3/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.5) (2.4)\nRequirement already satisfied: anyio<5,>=3 in /databricks/python3/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.16->langchain==0.1.5) (3.5.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (1.4.4)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (1.0.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (3.0.9)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (4.25.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow==2.10.1) (9.4.0)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk->unstructured[docx,pdf]==0.10.30) (1.2.0)\nRequirement already satisfied: sniffio in /databricks/python3/lib/python3.10/site-packages (from openai>=1.1.0->llama-index==0.9.3) (1.2.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index==0.9.3) (1.7.0)\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from httpx->llama-index==0.9.3) (2022.12.7)\nRequirement already satisfied: httpcore==1.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from httpx->llama-index==0.9.3) (1.0.5)\nRequirement already satisfied: idna in /databricks/python3/lib/python3.10/site-packages (from httpx->llama-index==0.9.3) (3.4)\nRequirement already satisfied: h11<0.15,>=0.13 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index==0.9.3) (0.14.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers==4.30.2) (2.0.4)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow==2.10.1) (2.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.5) (2.0.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /databricks/python3/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index==0.9.3) (0.4.3)\nWARNING: pandas 1.5.3 does not provide the extra 'jinja2'\nRequirement already satisfied: cryptography>=36.0.0 in /databricks/python3/lib/python3.10/site-packages (from pdfminer.six->unstructured[docx,pdf]==0.10.30) (39.0.1)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.10/site-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[docx,pdf]==0.10.30) (1.15.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow==2.10.1) (5.0.0)\nRequirement already satisfied: flatbuffers in /databricks/python3/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (23.5.26)\nRequirement already satisfied: coloredlogs in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (15.0.1)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.10/site-packages (from onnxruntime<1.16->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (1.11.1)\nRequirement already satisfied: pdfplumber in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (0.11.0)\nRequirement already satisfied: iopath in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (0.1.10)\nRequirement already satisfied: torchvision in /databricks/python3/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (0.15.2+cpu)\nRequirement already satisfied: effdet in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (0.4.1)\nRequirement already satisfied: torch in /databricks/python3/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (2.0.1+cpu)\nRequirement already satisfied: pytesseract in /databricks/python3/lib/python3.10/site-packages (from layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (0.3.10)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[docx,pdf]==0.10.30) (2.21)\nRequirement already satisfied: humanfriendly>=9.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from coloredlogs->onnxruntime<1.16->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (10.0)\nRequirement already satisfied: timm>=0.9.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (0.9.16)\nRequirement already satisfied: pycocotools>=2.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (2.0.7)\nRequirement already satisfied: omegaconf>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (2.3.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (2.8.4)\nRequirement already satisfied: portalocker in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from iopath->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (2.8.2)\nRequirement already satisfied: pypdfium2>=4.18.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from pdfplumber->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (4.29.0)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.10/site-packages (from sympy->onnxruntime<1.16->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (1.2.1)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /local_disk0/.ephemeral_nfs/envs/pythonEnv-acf9ee97-eb01-4c16-973b-73ca6644b30a/lib/python3.10/site-packages (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference==0.7.11->unstructured[docx,pdf]==0.10.30) (4.9.3)\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.30.2 \"unstructured[pdf,docx]==0.10.30\" langchain==0.1.5 llama-index==0.9.3 databricks-vectorsearch==0.22 pydantic==1.10.9 mlflow==2.10.1\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "173a92bc-5e4a-4b06-9e8b-6c895b70e0d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS kb_dir;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482b60ce-826e-4445-995d-1e9251760777",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helpers Function\n",
    "'''\n",
    "\n",
    "# %run ./00-init\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import requests\n",
    "import collections\n",
    "import os\n",
    " \n",
    "def download_file_from_git(dest, owner, repo, path):\n",
    "    def download_file(url, destination):\n",
    "      local_filename = url.split('/')[-1]\n",
    "      # NOTE the stream=True parameter below\n",
    "      with requests.get(url, stream=True) as r:\n",
    "          r.raise_for_status()\n",
    "          print('saving '+destination+'/'+local_filename)\n",
    "          with open(destination+'/'+local_filename, 'wb') as f:\n",
    "              for chunk in r.iter_content(chunk_size=8192): \n",
    "                  # If you have chunk encoded response uncomment if\n",
    "                  # and set chunk_size parameter to None.\n",
    "                  #if chunk: \n",
    "                  f.write(chunk)\n",
    "      return local_filename\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "      os.makedirs(dest)\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "    files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "    def download_to_dest(url):\n",
    "         download_file(url, dest)\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        collections.deque(executor.map(download_to_dest, files))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def upload_pdfs_to_volume(volume_path):\n",
    "  download_file_from_git(volume_path, \"databricks-demos\", \"dbdemos-dataset\", \"/llm/databricks-pdf-documentation\")\n",
    "\n",
    "def upload_dataset_to_volume(volume_path):\n",
    "  download_file_from_git(volume_path, \"databricks-demos\", \"dbdemos-dataset\", \"/llm/databricks-documentation\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#install poppler on the cluster (should be done by init scripts)\n",
    "def install_ocr_on_nodes():\n",
    "    \"\"\"\n",
    "    install poppler on the cluster (should be done by init scripts)\n",
    "    \"\"\"\n",
    "    # from pyspark.sql import SparkSession\n",
    "    import subprocess\n",
    "    num_workers = max(1,int(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterWorkers\")))\n",
    "    command = \"sudo rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/* && sudo apt-get clean && sudo apt-get update && sudo apt-get install poppler-utils tesseract-ocr -y\" \n",
    "    subprocess.check_output(command, shell=True)\n",
    "\n",
    "    def run_command(iterator):\n",
    "        for x in iterator:\n",
    "            yield subprocess.check_output(command, shell=True)\n",
    "\n",
    "    # spark = SparkSession.builder.getOrCreate()\n",
    "    data = spark.sparkContext.parallelize(range(num_workers), num_workers) \n",
    "    # Use mapPartitions to run command in each partition (worker)\n",
    "    output = data.mapPartitions(run_command)\n",
    "    try:\n",
    "        output.collect();\n",
    "        print(\"OCR libraries installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't install on all node: {e}\")\n",
    "        raise e\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def display_chat(chat_history, response):\n",
    "  def user_message_html(message):\n",
    "    return f\"\"\"\n",
    "      <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
    "        {message}\n",
    "      </div>\"\"\"\n",
    "  def assistant_message_html(message):\n",
    "    return f\"\"\"\n",
    "      <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
    "        <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
    "        {message}\n",
    "      </div>\"\"\"\n",
    "  chat_history_html = \"\".join([user_message_html(m[\"content\"]) if m[\"role\"] == \"user\" else assistant_message_html(m[\"content\"]) for m in chat_history])\n",
    "  answer = response[\"result\"].replace('\\n', '<br/>')\n",
    "  sources_html = (\"<br/><br/><br/><strong>Sources:</strong><br/> <ul>\" + '\\n'.join([f\"\"\"<li><a href=\"{s}\">{s}</a></li>\"\"\" for s in response[\"sources\"]]) + \"</ul>\") if response[\"sources\"] else \"\"\n",
    "  response_html = f\"\"\"{answer}{sources_html}\"\"\"\n",
    "\n",
    "  displayHTML(chat_history_html + assistant_message_html(response_html))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Temporary as we need routing to be in sdk\n",
    "class EndpointApiClient:\n",
    "    def __init__(self):\n",
    "        self.base_url =dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "        self.token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "        self.headers = {\"Authorization\": f\"Bearer {self.token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "    def create_inference_endpoint(self, endpoint_name, served_models, auto_capture_config = None):\n",
    "        data = {\"name\": endpoint_name, \"config\": {\"served_models\": served_models, \"auto_capture_config\": auto_capture_config}}\n",
    "        return self._post(\"api/2.0/serving-endpoints\", data)\n",
    "\n",
    "    def get_inference_endpoint(self, endpoint_name):\n",
    "        return self._get(f\"api/2.0/serving-endpoints/{endpoint_name}\", allow_error=True)\n",
    "      \n",
    "      \n",
    "    def inference_endpoint_exists(self, endpoint_name):\n",
    "      ep = self.get_inference_endpoint(endpoint_name)\n",
    "      if 'error_code' in ep and ep['error_code'] == 'RESOURCE_DOES_NOT_EXIST':\n",
    "          return False\n",
    "      if 'error_code' in ep and ep['error_code'] != 'RESOURCE_DOES_NOT_EXIST':\n",
    "          raise Exception(f\"enpoint exists ? {ep}\")\n",
    "      return True\n",
    "\n",
    "    def create_endpoint_if_not_exists(self, endpoint_name, model_name, model_version, workload_size, scale_to_zero_enabled=True, wait_start=True, auto_capture_config = None, environment_vars = {}):\n",
    "      models = [{\n",
    "            \"model_name\": model_name,\n",
    "            \"model_version\": model_version,\n",
    "            \"workload_size\": workload_size,\n",
    "            \"scale_to_zero_enabled\": scale_to_zero_enabled,\n",
    "            \"environment_vars\": environment_vars\n",
    "      }]\n",
    "      if not self.inference_endpoint_exists(endpoint_name):\n",
    "        r = self.create_inference_endpoint(endpoint_name, models, auto_capture_config)\n",
    "      #Make sure we have the proper version deployed\n",
    "      else:\n",
    "        ep = self.get_inference_endpoint(endpoint_name)\n",
    "        if 'pending_config' in ep:\n",
    "            self.wait_endpoint_start(endpoint_name)\n",
    "            ep = self.get_inference_endpoint(endpoint_name)\n",
    "        if 'pending_config' in ep:\n",
    "            model_deployed = ep['pending_config']['served_models'][0]\n",
    "            print(f\"Error with the model deployed: {model_deployed} - state {ep['state']}\")\n",
    "        else:\n",
    "            model_deployed = ep['config']['served_models'][0]\n",
    "        if model_deployed['model_version'] != model_version:\n",
    "          print(f\"Current model is version {model_deployed['model_version']}. Updating to {model_version}...\")\n",
    "          u = self.update_model_endpoint(endpoint_name, {\"served_models\": models})\n",
    "      if wait_start:\n",
    "        self.wait_endpoint_start(endpoint_name)\n",
    "      \n",
    "      \n",
    "    def list_inference_endpoints(self):\n",
    "        return self._get(\"api/2.0/serving-endpoints\")\n",
    "\n",
    "    def update_model_endpoint(self, endpoint_name, conf):\n",
    "        return self._put(f\"api/2.0/serving-endpoints/{endpoint_name}/config\", conf)\n",
    "\n",
    "    def delete_inference_endpoint(self, endpoint_name):\n",
    "        return self._delete(f\"api/2.0/serving-endpoints/{endpoint_name}\")\n",
    "\n",
    "    def wait_endpoint_start(self, endpoint_name):\n",
    "      i = 0\n",
    "      while self.get_inference_endpoint(endpoint_name)['state']['config_update'] == \"IN_PROGRESS\" and i < 500:\n",
    "        if i % 10 == 0:\n",
    "          print(\"waiting for endpoint to build model image and start...\")\n",
    "        time.sleep(10)\n",
    "        i += 1\n",
    "      ep = self.get_inference_endpoint(endpoint_name)\n",
    "      if ep['state'].get(\"ready\", None) != \"READY\":\n",
    "        print(f\"Error creating the endpoint: {ep}\")\n",
    "        \n",
    "      \n",
    "    # Making predictions\n",
    "\n",
    "    def query_inference_endpoint(self, endpoint_name, data):\n",
    "        return self._post(f\"realtime-inference/{endpoint_name}/invocations\", data)\n",
    "\n",
    "    # Debugging\n",
    "\n",
    "    def get_served_model_build_logs(self, endpoint_name, served_model_name):\n",
    "        return self._get(\n",
    "            f\"api/2.0/serving-endpoints/{endpoint_name}/served-models/{served_model_name}/build-logs\"\n",
    "        )\n",
    "\n",
    "    def get_served_model_server_logs(self, endpoint_name, served_model_name):\n",
    "        return self._get(\n",
    "            f\"api/2.0/serving-endpoints/{endpoint_name}/served-models/{served_model_name}/logs\"\n",
    "        )\n",
    "\n",
    "    def get_inference_endpoint_events(self, endpoint_name):\n",
    "        return self._get(f\"api/2.0/serving-endpoints/{endpoint_name}/events\")\n",
    "\n",
    "    def _get(self, uri, data = {}, allow_error = False):\n",
    "        r = requests.get(f\"{self.base_url}/{uri}\", params=data, headers=self.headers)\n",
    "        return self._process(r, allow_error)\n",
    "\n",
    "    def _post(self, uri, data = {}, allow_error = False):\n",
    "        return self._process(requests.post(f\"{self.base_url}/{uri}\", json=data, headers=self.headers), allow_error)\n",
    "\n",
    "    def _put(self, uri, data = {}, allow_error = False):\n",
    "        return self._process(requests.put(f\"{self.base_url}/{uri}\", json=data, headers=self.headers), allow_error)\n",
    "\n",
    "    def _delete(self, uri, data = {}, allow_error = False):\n",
    "        return self._process(requests.delete(f\"{self.base_url}/{uri}\", json=data, headers=self.headers), allow_error)\n",
    "\n",
    "    def _process(self, r, allow_error = False):\n",
    "      if r.status_code == 500 or r.status_code == 403 or not allow_error:\n",
    "        r.raise_for_status()\n",
    "      return r.json()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def send_requests_to_endpoint_and_wait_for_payload_to_be_available(endpoint_name, question_df, limit=50):\n",
    "  print(f'Sending {limit} requests to the endpoint {endpoint_name}, this will takes a few seconds...')\n",
    "  #send some requests\n",
    "  serving_client = EndpointApiClient()\n",
    "  def answer_question(question):\n",
    "    data = {\"messages\": [{\"role\": \"user\", \"content\": question}]}\n",
    "    answer = serving_client.query_inference_endpoint(endpoint_name, data)\n",
    "    return answer[0]\n",
    "\n",
    "  df_questions = question_df.limit(limit).toPandas()['question']\n",
    "  with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "      results = list(executor.map(answer_question, df_questions))\n",
    "  print(results)\n",
    "\n",
    "  #Wait for the inference table to be populated\n",
    "  print('Waiting for the inference to be in the Inference table, this can take a few seconds...')\n",
    "  from time import sleep\n",
    "  for i in range(10):\n",
    "    if table_exists(f'{endpoint_name}_payload') and not spark.table(f'{endpoint_name}_payload').count() < len(df_questions):\n",
    "      break\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f02366-7635-4f9c-b92c-8b52b22e6881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# DROP table devpost1.default.pdf_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c604b47-6c98-468c-a605-04b384636c72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/Volumes/devpost1/default/kb_dir/checkpoints/</td><td>checkpoints/</td><td>0</td><td>1715104594606</td></tr><tr><td>dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf</td><td>sample_pdf.pdf</td><td>21831</td><td>1715067002000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/Volumes/devpost1/default/kb_dir/checkpoints/",
         "checkpoints/",
         0,
         1715104594606
        ],
        [
         "dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf",
         "sample_pdf.pdf",
         21831,
         1715067002000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "volume_folder = '/Volumes/devpost1/default/kb_dir'\n",
    "display(dbutils.fs.ls(volume_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad0a3ae7-c359-4362-adae-b036e981a9b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.readStream\n",
    "        .format('cloudFiles')\n",
    "        .option('cloudFiles.format', 'BINARYFILE')\n",
    "        .option(\"pathGlobFilter\", \"*.pdf\")\n",
    "        .load('dbfs:'+volume_folder))\n",
    "\n",
    "# Write the data as a Delta table\n",
    "(df.writeStream\n",
    "  .trigger(availableNow=True)\n",
    "  .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/raw_docs')\n",
    "  .table('pdf_raw').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "832a5a11-0325-4e02-b3f3-a9d58dab27ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>modificationTime</th><th>length</th><th>content</th></tr></thead><tbody><tr><td>dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf</td><td>2024-05-07T07:30:02Z</td><td>21831</td><td>JVBERi0xLjQKJdPr6eEKMSAwIG9iago8PC9UaXRsZSAoVW50aXRsZWQgZG9jdW1lbnQpCi9Qcm9kdWNlciAoU2tpYS9QREYgbTEyNSBHb29nbGUgRG9jcyBSZW5kZXJlcik+PgplbmRvYmoKMyAwIG9iago8PC9jYSAxCi9CTSA= (truncated)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf",
         "2024-05-07T07:30:02Z",
         21831,
         "JVBERi0xLjQKJdPr6eEKMSAwIG9iago8PC9UaXRsZSAoVW50aXRsZWQgZG9jdW1lbnQpCi9Qcm9kdWNlciAoU2tpYS9QREYgbTEyNSBHb29nbGUgRG9jcyBSZW5kZXJlcik+PgplbmRvYmoKMyAwIG9iago8PC9jYSAxCi9CTSA= (truncated)"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 5
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "length",
         "type": "\"long\""
        },
        {
         "metadata": "{\"spark.contentAnnotation.mimeType\":\"application/pdf\"}",
         "name": "content",
         "type": "\"binary\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql SELECT * FROM pdf_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5517e18b-216b-4a6d-a33f-fee4a40b238e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For production use-case, install the libraries at your cluster level with an init script instead. \n",
    "# install_ocr_on_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b75338b-69a0-490b-b15b-8163b1d85d58",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-07 17:56:48.463496: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.auto import partition\n",
    "import re\n",
    "import io\n",
    "\n",
    "def extract_doc_text(x : bytes) -> str:\n",
    "  # Read files and extract the values with unstructured\n",
    "  sections = partition(file=io.BytesIO(x))\n",
    "  def clean_section(txt):\n",
    "    txt = re.sub(r'\\n', '', txt)\n",
    "    return re.sub(r' ?\\.', '.', txt)\n",
    "  # Default split is by section of document, concatenate them all together because we want to split by sentence instead.\n",
    "  return \"\\n\".join([clean_section(s.text) for s in sections]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f180c8d-6519-421b-bbd7-d1faf25bbedb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import io\n",
    "# import re\n",
    "# import requests\n",
    "# with requests.get('https://github.com/databricks-demos/dbdemos-dataset/blob/main/llm/databricks-pdf-documentation/Databricks-Customer-360-ebook-Final.pdf?raw=true') as pdf:\n",
    "#   doc = extract_doc_text(pdf.content)  \n",
    "#   print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a731d522-b762-43b6-87ce-c85618146266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.langchain_helpers.text_splitter import SentenceSplitter\n",
    "from llama_index import Document, set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "import pandas as pd\n",
    "\n",
    "# Reduce the arrow batch size as our PDF can be big in memory\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", 10)\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    #set llama2 as tokenizer to match our model size (will stay below BGE 1024 limit)\n",
    "    set_global_tokenizer(\n",
    "      AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    #Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    def extract_and_split(b):\n",
    "      txt = extract_doc_text(b)\n",
    "      nodes = splitter.get_nodes_from_documents([Document(text=txt)])\n",
    "      return [n.text for n in nodes]\n",
    "\n",
    "    for x in batch_iter:\n",
    "        yield x.apply(extract_and_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45212931-4062-41ba-8702-f9ccc946d553",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'a3c11a7a-d3e7-4afa-b6d6-48bda92fe02e', 'object': 'list', 'model': 'bge-large-en-v1.5', 'data': [{'index': 0, 'object': 'embedding', 'embedding': [0.0185699462890625, -0.01403045654296875, -0.057647705078125, 0.003448486328125, 0.008575439453125, -0.0216827392578125, -0.0247344970703125, -0.0047149658203125, 0.0136260986328125, 0.050323486328125, -0.027496337890625, -0.0147247314453125, 0.05474853515625, -0.053802490234375, -0.01025390625, -0.0161895751953125, -0.018768310546875, -0.017181396484375, -0.051177978515625, 0.0178680419921875, 0.0042877197265625, 0.028533935546875, -0.05548095703125, -0.037750244140625, -0.0012273788452148438, 0.0201873779296875, -0.0467529296875, 0.015869140625, 0.09375, 0.0194854736328125, -0.044708251953125, -0.01235198974609375, -0.00634765625, -0.0291900634765625, 0.04327392578125, -0.02532958984375, 0.049468994140625, 0.03240966796875, -0.06011962890625, -0.01251220703125, 0.0218963623046875, -0.00994873046875, 0.0134735107421875, -0.040435791015625, -0.07763671875, -0.04083251953125, 0.0031681060791015625, -0.033721923828125, -0.01293182373046875, -0.040985107421875, -0.039886474609375, 0.0231781005859375, 0.0033359527587890625, -0.0020122528076171875, 6.22868537902832e-05, 0.01079559326171875, -0.02056884765625, -0.0028896331787109375, -0.05841064453125, 0.00595855712890625, -0.00946807861328125, 0.044219970703125, 0.0177001953125, -0.060699462890625, -0.0022411346435546875, 0.05010986328125, -0.00711822509765625, -0.00415802001953125, 0.00875091552734375, -0.0269927978515625, -0.051177978515625, 0.0264892578125, -0.009674072265625, -0.023834228515625, -0.0267333984375, 0.027191162109375, 0.010467529296875, -0.0201873779296875, 0.00769805908203125, 0.04815673828125, -0.0102996826171875, 0.0179595947265625, -0.0218505859375, 0.0294342041015625, -0.0377197265625, -0.04559326171875, 0.07781982421875, -0.0088958740234375, -0.0029582977294921875, 0.020355224609375, -0.0209503173828125, 0.0245819091796875, 0.034759521484375, 0.0167694091796875, 0.006046295166015625, 0.051971435546875, -0.0372314453125, 0.047119140625, 0.0253143310546875, 0.048828125, 0.02520751953125, 0.0238037109375, -0.045135498046875, 0.016571044921875, 0.0016994476318359375, -0.005672454833984375, 0.0175018310546875, 0.01255035400390625, -0.042816162109375, -0.041412353515625, -0.00403594970703125, -0.0209503173828125, -0.0196990966796875, -0.01104736328125, 0.0049591064453125, 0.00917816162109375, -0.0134124755859375, 0.0245819091796875, 0.0098419189453125, -0.0574951171875, -0.01035308837890625, 0.00687408447265625, 0.02001953125, -0.0216827392578125, -0.041778564453125, -0.07391357421875, -0.01215362548828125, 0.047119140625, 0.0130615234375, 0.003559112548828125, 0.030975341796875, -0.0207977294921875, -0.06146240234375, 0.0192718505859375, 0.0016727447509765625, -0.02227783203125, 0.031982421875, 0.037322998046875, 0.047637939453125, -0.0161590576171875, -0.0283355712890625, 0.02105712890625, -0.032135009765625, 0.0782470703125, -0.01092529296875, 0.038787841796875, 0.0032367706298828125, -0.0206298828125, -0.0732421875, 0.053466796875, -0.0252532958984375, -0.022430419921875, 0.04888916015625, 0.0214385986328125, -0.013702392578125, 0.0325927734375, -0.01316070556640625, 0.024627685546875, 0.03851318359375, -0.06640625, 0.000431060791015625, 0.0116729736328125, -0.0213165283203125, -0.00295257568359375, -0.0237579345703125, 0.027984619140625, 0.0108489990234375, -0.01934814453125, -0.03436279296875, 0.01171112060546875, 0.0040283203125, 0.0012760162353515625, -0.040008544921875, 0.00037479400634765625, 0.06298828125, -0.01220703125, -0.0023288726806640625, -0.02777099609375, 0.0016984939575195312, 0.033966064453125, 4.57763671875e-05, -0.01190948486328125, -0.029205322265625, 0.0044403076171875, -0.0249176025390625, 0.0416259765625, 0.015960693359375, 0.01375579833984375, 0.00994110107421875, -0.01959228515625, 0.01041412353515625, 0.0887451171875, -0.051239013671875, 0.0240325927734375, -0.00429534912109375, -0.0086517333984375, -0.031982421875, 0.0033550262451171875, -0.01125335693359375, -0.03961181640625, 0.0007066726684570312, 0.01190185546875, 0.00916290283203125, -0.031890869140625, -0.0046844482421875, -0.032073974609375, 0.00855255126953125, 0.005634307861328125, -0.04949951171875, -0.03363037109375, 0.058624267578125, 0.033935546875, -0.00403594970703125, 0.0010223388671875, 0.04461669921875, -0.0229644775390625, -0.005390167236328125, 0.01396942138671875, 0.0124969482421875, -0.0029754638671875, -0.01093292236328125, 0.01580810546875, 0.03369140625, 0.060333251953125, -0.0198211669921875, -0.01284027099609375, -0.029693603515625, -0.0051422119140625, -0.032135009765625, -0.040618896484375, 0.035125732421875, 0.04296875, -0.011260986328125, 0.0166168212890625, -0.0014734268188476562, -0.020904541015625, -0.00934600830078125, 0.0457763671875, -0.040283203125, 0.0043792724609375, -0.0038204193115234375, 0.0144805908203125, 0.0008611679077148438, -0.0014257431030273438, 0.04901123046875, 0.007091522216796875, 0.023468017578125, -0.0277862548828125, -0.004238128662109375, 0.0433349609375, -0.020660400390625, 0.0228118896484375, 0.0272674560546875, 0.00943756103515625, -0.0213623046875, -0.01309967041015625, -0.038482666015625, 0.004486083984375, -0.021148681640625, -0.040985107421875, -0.00455474853515625, 0.018585205078125, -0.0003368854522705078, 0.003597259521484375, 0.0439453125, 0.042327880859375, 0.00997161865234375, -0.002689361572265625, -0.0027294158935546875, -0.0030803680419921875, -0.0237579345703125, -0.063720703125, -0.0016880035400390625, -0.0109405517578125, -0.06292724609375, 0.02203369140625, -0.01142120361328125, -0.0672607421875, 0.041046142578125, -0.002086639404296875, 0.00331878662109375, -0.019561767578125, -0.0022602081298828125, -0.0176849365234375, 0.032379150390625, -0.0109100341796875, -0.0264434814453125, -0.004619598388671875, -0.03277587890625, -0.00372314453125, -0.01207733154296875, 0.0015850067138671875, 0.0232696533203125, 0.026031494140625, 0.0095977783203125, -0.00024008750915527344, -0.0157928466796875, 0.0191802978515625, 0.01456451416015625, 0.0128936767578125, -0.033203125, -0.0078887939453125, -0.0179290771484375, -0.0036144256591796875, -0.0190582275390625, 0.018310546875, 0.0360107421875, -0.017974853515625, 0.045654296875, 0.0204010009765625, 0.023956298828125, 0.0028057098388671875, 0.00852203369140625, 0.0120086669921875, -0.02996826171875, 0.03216552734375, 0.00794219970703125, 0.0083465576171875, -0.01018524169921875, -0.01123809814453125, 0.0014333724975585938, 0.010467529296875, -0.0330810546875, -0.0286865234375, -0.0305633544921875, 0.059783935546875, -0.019744873046875, -0.06622314453125, -0.0188446044921875, 0.01088714599609375, -0.019439697265625, 0.0017070770263671875, -0.01230621337890625, 0.01468658447265625, 0.0188751220703125, 0.0306549072265625, 0.01282501220703125, 0.017669677734375, 0.038787841796875, -0.039947509765625, 0.015472412109375, -0.04736328125, 0.0181121826171875, 0.05157470703125, 0.032867431640625, 0.020721435546875, -0.0100860595703125, -0.0145111083984375, 0.042633056640625, -0.005046844482421875, -0.0653076171875, -0.012115478515625, 0.01369476318359375, 0.03143310546875, -0.014251708984375, 0.028717041015625, -0.039398193359375, -0.0169830322265625, -0.00920867919921875, -0.021514892578125, 0.045623779296875, 0.01114654541015625, -0.0233001708984375, -0.00872039794921875, 0.001567840576171875, -0.029541015625, -0.019805908203125, 0.00792694091796875, 0.071044921875, -0.04254150390625, 0.002605438232421875, 0.03118896484375, -0.01342010498046875, 0.07025146484375, -0.0010042190551757812, -0.02880859375, 0.041015625, -0.033660888671875, 0.0264434814453125, 0.000568389892578125, -0.00878143310546875, 0.028472900390625, 0.0224609375, 0.052032470703125, 0.05462646484375, 0.0256805419921875, -0.02880859375, -0.00751495361328125, -0.029998779296875, 0.0211334228515625, -0.03167724609375, 0.0268096923828125, 0.01544952392578125, 0.02630615234375, 0.002803802490234375, -0.055694580078125, 0.015625, 0.0280914306640625, 0.049896240234375, 0.0210418701171875, 0.03619384765625, 0.0167236328125, -0.0094757080078125, 0.0182037353515625, 0.0318603515625, -0.031280517578125, -0.02001953125, -0.0167694091796875, 0.03875732421875, -0.007656097412109375, -0.030975341796875, 0.0077362060546875, -0.025604248046875, -0.0101165771484375, 0.0196990966796875, 0.028228759765625, -0.01114654541015625, 0.02532958984375, 0.015625, 0.0595703125, 0.04864501953125, 0.0033283233642578125, -0.05987548828125, 0.0191802978515625, -0.01082611083984375, -0.0289154052734375, -0.01617431640625, -0.042449951171875, 0.01210784912109375, 0.065673828125, -0.009002685546875, -0.03759765625, -0.01029205322265625, -0.0296478271484375, -0.020904541015625, 0.02996826171875, 0.0270843505859375, -0.00705718994140625, 0.014495849609375, -0.06591796875, -0.01641845703125, 0.013763427734375, 0.0489501953125, -0.0038242340087890625, -0.0145111083984375, 0.01097869873046875, -0.0238800048828125, 0.023956298828125, 0.01074981689453125, 0.012969970703125, -0.036956787109375, -0.06719970703125, 0.0075531005859375, -0.025115966796875, -0.0333251953125, -0.014862060546875, 0.0182952880859375, -0.01428985595703125, 0.038299560546875, 0.00592041015625, -0.01059722900390625, -0.0004944801330566406, 0.0523681640625, -0.00994873046875, -0.0237579345703125, 0.0404052734375, -0.049530029296875, -0.00991058349609375, 0.050628662109375, 0.06414794921875, -0.0258331298828125, 0.03839111328125, -0.0006880760192871094, -0.0278167724609375, 0.0283355712890625, -0.036529541015625, 0.0235748291015625, -0.012542724609375, 0.0301361083984375, -0.036651611328125, -0.00612640380859375, 0.03125, -0.00962066650390625, 0.026031494140625, 0.0118560791015625, -0.056182861328125, 0.0037403106689453125, 0.01053619384765625, -0.026214599609375, 0.0243988037109375, -0.01153564453125, 0.0005435943603515625, 0.03363037109375, -0.0194091796875, 0.01442718505859375, -0.021331787109375, 0.05206298828125, 0.0243377685546875, 0.0185394287109375, 0.040283203125, 0.0721435546875, -0.05084228515625, -0.0787353515625, 0.01033782958984375, -0.023193359375, -0.0009512901306152344, -0.0167694091796875, 0.05401611328125, 0.0275115966796875, 0.01108551025390625, -0.0079803466796875, -0.0174102783203125, -0.0419921875, 0.00038814544677734375, -0.028228759765625, 0.0014200210571289062, 0.0433349609375, -0.0421142578125, -0.032623291015625, 0.04412841796875, 0.0022563934326171875, -0.052886962890625, -0.039825439453125, 0.0189971923828125, -0.01568603515625, 0.00780487060546875, 0.05377197265625, 0.004688262939453125, 0.0155181884765625, -0.06414794921875, 0.035369873046875, -0.024688720703125, 0.004913330078125, 0.01419830322265625, 0.019439697265625, -0.04229736328125, 0.02484130859375, 0.050506591796875, -0.032012939453125, 0.0153961181640625, 0.0070648193359375, -0.0219879150390625, -0.01568603515625, -0.0175323486328125, -0.00970458984375, 0.030975341796875, 0.021209716796875, 0.035614013671875, -0.0223236083984375, 0.01090240478515625, 0.01132965087890625, 0.02874755859375, -0.01187896728515625, 0.050506591796875, -0.07373046875, -0.004367828369140625, 0.012786865234375, -0.016448974609375, 0.0222930908203125, 0.0180511474609375, -0.04339599609375, 0.044708251953125, 0.0026645660400390625, -0.0071868896484375, -0.0164337158203125, -0.0036067962646484375, -0.0260162353515625, -0.022735595703125, -0.0129852294921875, -0.01605224609375, 0.032196044921875, 0.04351806640625, 0.0261383056640625, 0.028289794921875, -0.0019464492797851562, -0.044403076171875, 0.0132598876953125, -0.03704833984375, -0.084228515625, 0.0240478515625, -0.00705718994140625, -0.00975799560546875, -0.0018491744995117188, 0.0033588409423828125, -0.006366729736328125, -0.024810791015625, 0.013092041015625, 0.030029296875, 0.0295562744140625, 0.040069580078125, -0.00957489013671875, 0.0011529922485351562, 0.03375244140625, -0.03570556640625, -0.0187225341796875, -0.0163116455078125, 0.0020160675048828125, -0.0253143310546875, -0.0765380859375, -0.0211334228515625, -0.0157928466796875, -0.006046295166015625, 0.0235137939453125, -0.055389404296875, -0.005252838134765625, 0.001888275146484375, -0.01605224609375, -0.039947509765625, 0.040557861328125, -0.01434326171875, -0.0100555419921875, 0.083740234375, 0.00421142578125, -0.0110015869140625, 0.0474853515625, -0.026885986328125, 0.046630859375, -0.04779052734375, 0.03839111328125, -0.0137176513671875, -0.01528167724609375, -0.0012807846069335938, 0.00482940673828125, -0.05511474609375, -0.03521728515625, -0.0003199577331542969, 0.004180908203125, -0.01416778564453125, -0.0171051025390625, -0.006496429443359375, -0.051025390625, -0.0574951171875, -0.01328277587890625, 0.021514892578125, 0.01776123046875, 0.002819061279296875, 0.0262298583984375, 0.020233154296875, -0.00421905517578125, 0.021759033203125, 0.0589599609375, -0.00012493133544921875, 0.0219573974609375, -0.0243377685546875, 0.0181884765625, -0.0185699462890625, -0.00994873046875, 0.020660400390625, -0.02294921875, 0.020477294921875, -0.0167999267578125, 0.062744140625, 0.058380126953125, -0.0204925537109375, 0.0003693103790283203, 0.0216827392578125, -0.0146026611328125, -0.07904052734375, -0.0020732879638671875, 0.01013946533203125, -0.0033130645751953125, 0.042388916015625, -0.0279388427734375, -0.009765625, 0.016082763671875, 0.0213470458984375, -0.0259246826171875, -0.050933837890625, -0.014892578125, -0.061859130859375, -0.019683837890625, -0.07275390625, -0.01195526123046875, -0.0269622802734375, 0.004489898681640625, 0.0027179718017578125, -0.0226898193359375, -0.0007872581481933594, 0.019134521484375, -0.0313720703125, -0.0189208984375, -0.0357666015625, -0.031036376953125, -0.005462646484375, -0.019683837890625, -0.030487060546875, -0.03143310546875, 0.03759765625, 0.014404296875, -0.039703369140625, 0.007965087890625, -0.00029397010803222656, 0.04107666015625, -0.00963592529296875, 0.0109405517578125, -0.062286376953125, -0.032012939453125, -0.0160980224609375, 0.0005631446838378906, 0.00034117698669433594, 0.007602691650390625, 0.015380859375, -0.03271484375, -0.017059326171875, 0.03564453125, 0.0022220611572265625, -0.021881103515625, -0.0012969970703125, 0.0090179443359375, -0.06494140625, -0.043426513671875, -0.02642822265625, 0.040435791015625, -0.08148193359375, -0.007297515869140625, 0.0102386474609375, 0.0029888153076171875, -0.0223388671875, 0.0318603515625, -0.0012254714965820312, 0.0200653076171875, -0.034088134765625, 0.0234375, 0.027008056640625, -0.0242919921875, 0.006717681884765625, 0.0098419189453125, 0.0535888671875, -0.03369140625, -0.006984710693359375, 0.0013093948364257812, 0.00865936279296875, -0.02899169921875, 0.034820556640625, 0.0170135498046875, 0.0008602142333984375, 0.0213165283203125, 0.0038242340087890625, -0.03179931640625, 0.0258941650390625, 0.04559326171875, 0.0163116455078125, 0.04852294921875, 0.03900146484375, -0.01361083984375, -0.0169830322265625, 0.02069091796875, -0.03387451171875, -0.0408935546875, 0.044464111328125, -0.072021484375, 0.06280517578125, -0.023773193359375, -0.01067352294921875, -0.05517578125, 0.01020050048828125, -0.026458740234375, 0.004329681396484375, -0.0010528564453125, 0.039825439453125, -0.0027313232421875, 0.0318603515625, -0.013336181640625, -0.00844573974609375, 0.05853271484375, -0.03985595703125, -0.0195465087890625, -0.00997161865234375, 0.01800537109375, 0.0036945343017578125, -0.035125732421875, 0.0086517333984375, -0.01226043701171875, -0.042144775390625, 0.051849365234375, -0.03387451171875, -0.004581451416015625, -0.043243408203125, -0.004245758056640625, -0.0301361083984375, 0.004199981689453125, -0.0252532958984375, 0.03106689453125, 0.063720703125, 0.0036754608154296875, -0.00646209716796875, 0.02587890625, 0.0570068359375, 0.044189453125, 0.01788330078125, 0.00011867284774780273, 0.004276275634765625, 0.0191497802734375, -0.062225341796875, -0.01922607421875, 0.03155517578125, 0.0692138671875, 0.053009033203125, 0.03271484375, 0.005157470703125, 0.06329345703125, 0.0103912353515625, 0.022064208984375, 0.021820068359375, 0.0159149169921875, -0.02923583984375, 0.04339599609375, -0.0244293212890625, -0.0013685226440429688, 0.00424957275390625, 0.0047454833984375, 0.03314208984375, 0.022125244140625, 0.0037021636962890625, -0.015869140625, 0.029296875, -0.016693115234375, 0.0035076141357421875, -0.00807952880859375, -0.0208740234375, 0.01812744140625, 0.0104522705078125, 0.034637451171875, -0.0755615234375, -0.07427978515625, -4.398822784423828e-05, 0.048858642578125, 0.0087432861328125, 0.005741119384765625, 0.00629425048828125, 0.028961181640625, -0.019866943359375, 0.061859130859375, -0.01122283935546875, 0.0300750732421875, 0.001987457275390625, 0.01776123046875, -0.033294677734375, 0.04217529296875, 0.0290679931640625, -0.0223388671875, 0.0213470458984375, 0.00795745849609375, -0.040374755859375, -0.0109405517578125, -0.0070037841796875, -0.048065185546875, 0.0390625, 0.036651611328125, -0.01329803466796875, 0.003528594970703125, 0.01207733154296875, -0.034088134765625, 0.022552490234375, 0.01050567626953125, 0.00039649009704589844, -0.022430419921875, 0.0352783203125, 0.0201568603515625, -0.0005526542663574219, -0.0019426345825195312, 0.023162841796875, -0.03857421875, 0.0009627342224121094, -0.00447845458984375, -0.05059814453125, 0.005115509033203125, -0.01052093505859375, -0.06658935546875, -0.0052032470703125, 0.032135009765625, -0.048065185546875, -0.002674102783203125, 0.07373046875, 0.01103973388671875, -0.0221405029296875, -0.00527191162109375, 0.021728515625, -0.006458282470703125, -0.06207275390625, -0.02301025390625, -0.009033203125, -0.0204315185546875, -0.03564453125, -0.0149993896484375, 0.01334381103515625, -0.0168609619140625, -0.013153076171875, 0.005615234375, -0.018218994140625, 0.00019359588623046875, -0.04901123046875, 0.0288543701171875, 0.00966644287109375, 0.0254974365234375, 0.040618896484375, 0.03936767578125, 0.01006317138671875, 0.0047149658203125, -0.0036716461181640625, 0.020355224609375, 0.08465576171875, -0.0210418701171875, 0.048370361328125, 0.037384033203125, 0.010833740234375, -0.0306549072265625, 0.01470184326171875, 0.0115966796875, -0.031402587890625, 0.0029239654541015625, -0.06005859375, 0.0018815994262695312, -0.0135955810546875, -0.0311737060546875, 0.0010194778442382812, 0.0198974609375, 0.0160675048828125, -0.0386962890625, -0.0086669921875, 0.01506805419921875, -0.0136871337890625, -0.036376953125, -0.055816650390625, 0.016143798828125, -0.0123138427734375, -0.0643310546875, 0.002166748046875, -0.05523681640625, 0.2108154296875, 0.0215911865234375, 0.0078277587890625, -0.00823211669921875, -0.0100860595703125, 0.01479339599609375, 0.05609130859375, -0.0352783203125, 0.033447265625, 0.00518035888671875, -0.00450897216796875, -0.0016832351684570312, 0.003955841064453125, 0.05023193359375, -0.0416259765625, 0.055633544921875, -0.0265045166015625, 0.0311126708984375, 0.0124969482421875, -0.0595703125, -0.07598876953125, -0.007110595703125, -0.0016307830810546875, 0.07501220703125, 0.0167083740234375, -0.0084686279296875, 0.0291290283203125, -0.043121337890625, -0.0030651092529296875, 0.0087738037109375, 0.0003101825714111328, -0.064208984375, -0.0009188652038574219, 0.033355712890625, -0.0205841064453125, 0.045989990234375, 0.06756591796875, -0.00762939453125, 0.0023975372314453125, 0.0548095703125, 0.015380859375, 0.0029449462890625, 0.0157012939453125, -0.0241546630859375, 0.0018320083618164062, 0.0275421142578125, -0.01071929931640625, -0.040130615234375, 0.03387451171875, -0.030029296875, 0.0019550323486328125, -0.04571533203125, 0.0013113021850585938, -0.004734039306640625, -0.05322265625, 0.007534027099609375, -0.034393310546875, -0.00868988037109375, -0.0260467529296875, -0.0521240234375, 0.047515869140625, 0.0116119384765625, -0.009674072265625, -0.00724029541015625, 0.009765625, -0.04119873046875, 0.0124053955078125, -0.043365478515625, -0.0164947509765625, -0.00296783447265625, 0.011474609375, 0.005153656005859375, -0.0013380050659179688, -0.037841796875, -0.0234832763671875, 0.069091796875, -0.01390838623046875, 0.043060302734375, -0.027069091796875, 0.0177154541015625, 0.056396484375, 0.00070953369140625, -0.0372314453125, -0.0252685546875, 0.033660888671875, 0.0305938720703125, -0.0531005859375, 0.0100555419921875, 0.0064239501953125, 0.004001617431640625, 0.01300811767578125, 0.0008106231689453125, 0.04302978515625, 0.033416748046875, 0.0084686279296875]}], 'usage': {'prompt_tokens': 7, 'total_tokens': 7}}\n"
     ]
    }
   ],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# bge-large-en Foundation models are available using the /serving-endpoints/databricks-bge-large-en/invocations api. \n",
    "deploy_client = get_deploy_client(\"databricks\")\n",
    "\n",
    "## NOTE: if you change your embedding model here, make sure you change it in the query step too\n",
    "embeddings = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [\"What is Apache Spark?\"]})\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c9ec9f-27ed-44e9-a598-3b5fa85b06ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--Note that we need to enable Change Data Feed on the table to create the index\n",
    "CREATE TABLE IF NOT EXISTS kb_vector_db (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  url STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY <FLOAT>\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a8f0e51-5b5b-4253-988b-1a14978a8d11",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE kb_vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95abbb59-6c50-4080-8088-492e2d19cdf9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(\"array<float>\")\n",
    "def get_embedding(contents: pd.Series) -> pd.Series:\n",
    "    import mlflow.deployments\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    def get_embeddings(batch):\n",
    "        #Note: this will fail if an exception is thrown during embedding creation (add try/except if needed) \n",
    "        response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": batch})\n",
    "        return [e['embedding'] for e in response.data]\n",
    "\n",
    "    # Splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.\n",
    "    max_batch_size = 150\n",
    "    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]\n",
    "\n",
    "    # Process each batch and collect the results\n",
    "    all_embeddings = []\n",
    "    for batch in batches:\n",
    "        all_embeddings += get_embeddings(batch.tolist())\n",
    "\n",
    "    return pd.Series(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06b27e33-15e9-4ca9-8ce8-c425944fc856",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog = spark.catalog\n",
    "catalog='devpost1'\n",
    "db = 'default'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83f39186-49ff-4789-87f0-a6dc358f1da9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "# from pyspark.sql.catalog import tableExists\n",
    "\n",
    "# spark.conf.set(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "\n",
    "# catalog = spark.catalog\n",
    "\n",
    "(spark.readStream.table('pdf_raw')\n",
    "      .withColumn(\"content\", F.explode(read_as_chunk(\"content\")))\n",
    "      .withColumn(\"embedding\", get_embedding(\"content\"))\n",
    "      .selectExpr('path as url', 'content', 'embedding')\n",
    "  .writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/pdf_chunk')\n",
    "    .table('kb_vector_db').awaitTermination())\n",
    "\n",
    "# #Let's also add our documentation web page from the simple demo (make sure you run the quickstart demo first)\n",
    "# catalog='devpost1'\n",
    "# db = 'default'\n",
    "# if catalog.tableExists(f'{catalog}.{db}.kb_vector'):\n",
    "#   (spark.readStream.option(\"skipChangeCommits\", \"true\").table('kb_vector') #skip changes for more stable demo\n",
    "#       .withColumn('embedding', get_embedding(\"content\"))\n",
    "#       .select('url', 'content', 'embedding')\n",
    "#   .writeStream\n",
    "#     .trigger(availableNow=True)\n",
    "#     .option(\"checkpointLocation\", f'dbfs:{volume_folder}/checkpoints/docs_chunks')\n",
    "#     .table('kb_vector_db').awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23106f75-3ee0-4e26-9f57-c4953e1aeb08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.catalog.listTables('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadb209e-befd-4d1f-9bb0-e26961bcd60f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>url</th><th>content</th><th>embedding</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 15
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "url",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "content",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "embedding",
         "type": "{\"type\":\"array\",\"elementType\":\"float\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM kb_vector_db WHERE url like '%.pdf' limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60a0cc2-629b-493b-b857-55c2f2dc245f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helpers Function 2\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, udf, length, pandas_udf\n",
    "import os\n",
    "import mlflow\n",
    "from typing import Iterator\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "\n",
    "import re\n",
    "min_required_version = \"11.3\"\n",
    "version_tag = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\")\n",
    "version_search = re.search('^([0-9]*\\.[0-9]*)', version_tag)\n",
    "assert version_search, f\"The Databricks version can't be extracted from {version_tag}, shouldn't happen, please correct the regex\"\n",
    "current_version = float(version_search.group(1))\n",
    "assert float(current_version) >= float(min_required_version), f'The Databricks version of the cluster must be >= {min_required_version}. Current version detected: {current_version}'\n",
    "\n",
    "# Helper function\n",
    "def get_latest_model_version(model_name):\n",
    "    mlflow_client = MlflowClient(registry_uri=\"databricks-uc\")\n",
    "    latest_version = 1\n",
    "    for mv in mlflow_client.search_model_versions(f\"name='{model_name}'\"):\n",
    "        version_int = int(mv.version)\n",
    "        if version_int > latest_version:\n",
    "            latest_version = version_int\n",
    "    return latest_version\n",
    "\n",
    "# COMMAND ----------\n",
    "import time\n",
    "\n",
    "def endpoint_exists(vsc, vs_endpoint_name):\n",
    "  try:\n",
    "    return vs_endpoint_name in [e['name'] for e in vsc.list_endpoints().get('endpoints', [])]\n",
    "  except Exception as e:\n",
    "    #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "    if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "      print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. The demo will consider it exists\")\n",
    "      return True\n",
    "    else:\n",
    "      raise e\n",
    "\n",
    "def wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name):\n",
    "  for i in range(180):\n",
    "    try:\n",
    "      endpoint = vsc.get_endpoint(vs_endpoint_name)\n",
    "    except Exception as e:\n",
    "      #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "      if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "        print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. Please manually check your endpoint status\")\n",
    "        return\n",
    "      else:\n",
    "        raise e\n",
    "    status = endpoint.get(\"endpoint_status\", endpoint.get(\"status\"))[\"state\"].upper()\n",
    "    if \"ONLINE\" in status:\n",
    "      return endpoint\n",
    "    elif \"PROVISIONING\" in status or i <6:\n",
    "      if i % 20 == 0: \n",
    "        print(f\"Waiting for endpoint to be ready, this can take a few min... {endpoint}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "      raise Exception(f'''Error with the endpoint {vs_endpoint_name}. - this shouldn't happen: {endpoint}.\\n Please delete it and re-run the previous cell: vsc.delete_endpoint(\"{vs_endpoint_name}\")''')\n",
    "  raise Exception(f\"Timeout, your endpoint isn't ready yet: {vsc.get_endpoint(vs_endpoint_name)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,index\n",
    "def index_exists(vsc, endpoint_name, index_full_name):\n",
    "    try:\n",
    "        dict_vsindex = vsc.get_index(endpoint_name, index_full_name).describe()\n",
    "        return dict_vsindex.get('status').get('ready', False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if 'RESOURCE_DOES_NOT_EXIST' not in str(e):\n",
    "            print(str(e))\n",
    "            print(f'Unexpected error describing the index. This could be a permission issue.')\n",
    "        return False\n",
    "        # raise e\n",
    "    \n",
    "def wait_for_index_to_be_ready(vsc, vs_endpoint_name, index_name):\n",
    "  for i in range(180):\n",
    "    idx = vsc.get_index(vs_endpoint_name, index_name).describe()\n",
    "    index_status = idx.get('status', idx.get('index_status', {}))\n",
    "    status = index_status.get('detailed_state', index_status.get('status', 'UNKNOWN')).upper()\n",
    "    url = index_status.get('index_url', index_status.get('url', 'UNKNOWN'))\n",
    "    if \"ONLINE\" in status:\n",
    "      return\n",
    "    if \"UNKNOWN\" in status:\n",
    "      print(f\"Can't get the status - will assume index is ready {idx} - url: {url}\")\n",
    "      return\n",
    "    elif \"PROVISIONING\" in status:\n",
    "      if i % 40 == 0: print(f\"Waiting for index to be ready, this can take a few min... {index_status} - pipeline url:{url}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "        raise Exception(f'''Error with the index - this shouldn't happen. DLT pipeline might have been killed.\\n Please delete it and re-run the previous cell: vsc.delete_index(\"{index_name}, {vs_endpoint_name}\") \\nIndex details: {idx}''')\n",
    "  raise Exception(f\"Timeout, your index isn't ready yet: {vsc.get_index(index_name, vs_endpoint_name)}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql.types import StringType\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "#Add retries with backoff to avoid 429 while fetching the doc\n",
    "retries = Retry(\n",
    "    total=3,\n",
    "    backoff_factor=3,\n",
    "    status_forcelist=[429],\n",
    ")\n",
    "\n",
    "def download_databricks_documentation_articles(max_documents=None):\n",
    "    # Fetch the XML content from sitemap\n",
    "    response = requests.get(DATABRICKS_SITEMAP_URL)\n",
    "    root = ET.fromstring(response.content)\n",
    "\n",
    "    # Find all 'loc' elements (URLs) in the XML\n",
    "    urls = [loc.text for loc in root.findall(\".//{http://www.sitemaps.org/schemas/sitemap/0.9}loc\")]\n",
    "    if max_documents:\n",
    "        urls = urls[:max_documents]\n",
    "\n",
    "    # Create DataFrame from URLs\n",
    "    df_urls = spark.createDataFrame(urls, StringType()).toDF(\"url\").repartition(10)\n",
    "\n",
    "    # Pandas UDF to fetch HTML content for a batch of URLs\n",
    "    @pandas_udf(\"string\")\n",
    "    def fetch_html_udf(urls: pd.Series) -> pd.Series:\n",
    "        adapter = HTTPAdapter(max_retries=retries)\n",
    "        http = requests.Session()\n",
    "        http.mount(\"http://\", adapter)\n",
    "        http.mount(\"https://\", adapter)\n",
    "        def fetch_html(url):\n",
    "            try:\n",
    "                response = http.get(url)\n",
    "                if response.status_code == 200:\n",
    "                    return response.content\n",
    "            except requests.RequestException:\n",
    "                return None\n",
    "            return None\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=200) as executor:\n",
    "            results = list(executor.map(fetch_html, urls))\n",
    "        return pd.Series(results)\n",
    "\n",
    "    # Pandas UDF to process HTML content and extract text\n",
    "    @pandas_udf(\"string\")\n",
    "    def download_web_page_udf(html_contents: pd.Series) -> pd.Series:\n",
    "        def extract_text(html_content):\n",
    "            if html_content:\n",
    "                soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "                article_div = soup.find(\"div\", itemprop=\"articleBody\")\n",
    "                if article_div:\n",
    "                    return str(article_div).strip()\n",
    "            return None\n",
    "\n",
    "        return html_contents.apply(extract_text)\n",
    "\n",
    "    # Apply UDFs to DataFrame\n",
    "    df_with_html = df_urls.withColumn(\"html_content\", fetch_html_udf(\"url\"))\n",
    "    final_df = df_with_html.withColumn(\"text\", download_web_page_udf(\"html_content\"))\n",
    "\n",
    "    # Select and filter non-null results\n",
    "    final_df = final_df.select(\"url\", \"text\").filter(\"text IS NOT NULL\").cache()\n",
    "    if final_df.isEmpty():\n",
    "      raise Exception(\"Dataframe is empty, couldn't download Databricks documentation, please check sitemap status.\")\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def display_gradio_app(space_name = \"databricks-demos-chatbot\"):\n",
    "    displayHTML(f'''<div style=\"margin: auto; width: 1000px\"><iframe src=\"https://{space_name}.hf.space\" frameborder=\"0\" width=\"1000\" height=\"950\" style=\"margin: auto\"></iframe></div>''')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Cleanup utility to remove demo assets\n",
    "def cleanup_demo(catalog, db, serving_endpoint_name, vs_index_fullname):\n",
    "  vsc = VectorSearchClient()\n",
    "  try:\n",
    "    vsc.delete_index(endpoint_name = VECTOR_SEARCH_ENDPOINT_NAME, index_name=vs_index_fullname)\n",
    "  except Exception as e:\n",
    "    print(f\"can't delete index {VECTOR_SEARCH_ENDPOINT_NAME} {vs_index_fullname} - might not be existing: {e}\")\n",
    "  try:\n",
    "    WorkspaceClient().serving_endpoints.delete(serving_endpoint_name)\n",
    "  except Exception as e:\n",
    "    print(f\"can't delete serving endpoint {serving_endpoint_name} - might not be existing: {e}\")\n",
    "  spark.sql(f'DROP SCHEMA `{catalog}`.`{db}` CASCADE')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Demo helper to debug permission issue\n",
    "def test_demo_permissions(host, secret_scope, secret_key, vs_endpoint_name, index_name, embedding_endpoint_name = None, managed_embeddings = True):\n",
    "  error = False\n",
    "  CSS_REPORT = \"\"\"\n",
    "  <style>\n",
    "  .dbdemos_install{\n",
    "                      font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji,FontAwesome;\n",
    "  color: #3b3b3b;\n",
    "  box-shadow: 0 .15rem 1.15rem 0 rgba(58,59,69,.15)!important;\n",
    "  padding: 10px 20px 20px 20px;\n",
    "  margin: 10px;\n",
    "  font-size: 14px !important;\n",
    "  }\n",
    "  .dbdemos_block{\n",
    "      display: block !important;\n",
    "      width: 900px;\n",
    "  }\n",
    "  .code {\n",
    "      padding: 5px;\n",
    "      border: 1px solid #e4e4e4;\n",
    "      font-family: monospace;\n",
    "      background-color: #f5f5f5;\n",
    "      margin: 5px 0px 0px 0px;\n",
    "      display: inline;\n",
    "  }\n",
    "  </style>\"\"\"\n",
    "\n",
    "  def display_error(title, error, color=\"\"):\n",
    "    displayHTML(f\"\"\"{CSS_REPORT}\n",
    "      <div class=\"dbdemos_install\">\n",
    "                          <h1 style=\"color: #eb0707\">Configuration error: {title}</h1> \n",
    "                            {error}\n",
    "                        </div>\"\"\")\n",
    "  \n",
    "  def get_email():\n",
    "    try:\n",
    "      return spark.sql('select current_user() as user').collect()[0]['user']\n",
    "    except:\n",
    "      return 'Uknown'\n",
    "\n",
    "  def get_token_error(msg, e):\n",
    "    return f\"\"\"\n",
    "    {msg}<br/><br/>\n",
    "    Your model will be served using Databrick Serverless endpoint and needs a Pat Token to authenticate.<br/>\n",
    "    <strong> This must be saved as a secret to be accessible when the model is deployed.</strong><br/><br/>\n",
    "    Here is how you can add the Pat Token as a secret available within your notebook and for the model:\n",
    "    <ul>\n",
    "    <li>\n",
    "      first, setup the Databricks CLI on your laptop or using this cluster terminal:\n",
    "      <div class=\"code dbdemos_block\">pip install databricks-cli</div>\n",
    "    </li>\n",
    "    <li> \n",
    "      Configure the CLI. You'll need your workspace URL and a PAT token from your profile page\n",
    "      <div class=\"code dbdemos_block\">databricks configure</div>\n",
    "    </li>  \n",
    "    <li>\n",
    "      Create the dbdemos scope:\n",
    "      <div class=\"code dbdemos_block\">databricks secrets create-scope dbdemos</div>\n",
    "    <li>\n",
    "      Save your service principal secret. It will be used by the Model Endpoint to autenticate. <br/>\n",
    "      If this is a demo/test, you can use one of your PAT token.\n",
    "      <div class=\"code dbdemos_block\">databricks secrets put-secret dbdemos rag_sp_token</div>\n",
    "    </li>\n",
    "    <li>\n",
    "      Optional - if someone else created the scope, make sure they give you read access to the secret:\n",
    "      <div class=\"code dbdemos_block\">databricks secrets put-acl dbdemos '{get_email()}' READ</div>\n",
    "\n",
    "    </li>  \n",
    "    </ul>  \n",
    "    <br/>\n",
    "    Detailed error trying to access the secret:\n",
    "      <div class=\"code dbdemos_block\">{e}</div>\"\"\"\n",
    "\n",
    "  try:\n",
    "    secret = dbutils.secrets.get(secret_scope, secret_key)\n",
    "    secret_principal = \"__UNKNOWN__\"\n",
    "    try:\n",
    "      from databricks.sdk import WorkspaceClient\n",
    "      w = WorkspaceClient(token=dbutils.secrets.get(secret_scope, secret_key), host=host)\n",
    "      secret_principal = w.current_user.me().emails[0].value\n",
    "    except Exception as e_sp:\n",
    "      error = True\n",
    "      display_error(f\"Couldn't get the SP identity using the Pat Token saved in your secret\", \n",
    "                    get_token_error(f\"<strong>This likely means that the Pat Token saved in your secret {secret_scope}/{secret_key} is incorrect or expired. Consider replacing it.</strong>\", e_sp))\n",
    "      return\n",
    "  except Exception as e:\n",
    "    error = True\n",
    "    display_error(f\"We couldn't access the Pat Token saved in the secret {secret_scope}/{secret_key}\", \n",
    "                  get_token_error(\"<strong>This likely means your secret isn't set or not accessible for your user</strong>.\", e))\n",
    "    return\n",
    "  \n",
    "  try:\n",
    "    from databricks.vector_search.client import VectorSearchClient\n",
    "    vsc = VectorSearchClient(workspace_url=host, personal_access_token=secret, disable_notice=True)\n",
    "    vs_index = vsc.get_index(endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=index_name)\n",
    "    if embedding_endpoint_name:\n",
    "      if managed_embeddings:\n",
    "        from langchain_community.embeddings import DatabricksEmbeddings\n",
    "        results = vs_index.similarity_search(query_text='What is Apache Spark?', columns=[\"content\"], num_results=1)\n",
    "      else:\n",
    "        from langchain_community.embeddings import DatabricksEmbeddings\n",
    "        embedding_model = DatabricksEmbeddings(endpoint=embedding_endpoint_name)\n",
    "        embeddings = embedding_model.embed_query('What is Apache Spark?')\n",
    "        results = vs_index.similarity_search(query_vector=embeddings, columns=[\"content\"], num_results=1)\n",
    "\n",
    "  except Exception as e:\n",
    "    error = True\n",
    "    vs_error = f\"\"\"\n",
    "    Why are we getting this error?<br/>\n",
    "    The model is using the Pat Token saved with the secret {secret_scope}/{secret_key} to access your vector search index '{index_name}' (host:{host}).<br/><br/>\n",
    "    To do so, the principal owning the Pat Token must have USAGE permission on your schema and READ permission on the index.<br/>\n",
    "    The principal is the one who generated the token you saved as secret: `{secret_principal}`. <br/>\n",
    "    <i>Note: Production-grade deployement should to use a Service Principal ID instead.</i><br/>\n",
    "    <br/>\n",
    "    Here is how you can fix it:<br/><br/>\n",
    "    <strong>Make sure your Service Principal has USE privileve on the schema</strong>:\n",
    "    <div class=\"code dbdemos_block\">\n",
    "    spark.sql('GRANT USAGE ON CATALOG `{catalog}` TO `{secret_principal}>`');<br/>\n",
    "    spark.sql('GRANT USAGE ON DATABASE `{catalog}`.`{db}` TO `{secret_principal}`');<br/>\n",
    "    </div>\n",
    "    <br/>\n",
    "    <strong>Grant SELECT access to your SP to your index:</strong>\n",
    "    <div class=\"code dbdemos_block\">\n",
    "    from databricks.sdk import WorkspaceClient<br/>\n",
    "    import databricks.sdk.service.catalog as c<br/>\n",
    "    WorkspaceClient().grants.update(c.SecurableType.TABLE, \"{index_name}\",<br/>\n",
    "                                            changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=\"{secret_principal}\")])\n",
    "    </div>\n",
    "    <br/>\n",
    "    <strong>If this is still not working, make sure the value saved in your {secret_scope}/{secret_key} secret is your SP pat token </strong>.<br/>\n",
    "    <i>Note: if you're using a shared demo workspace, please do not change the secret value if was set to a valid SP value by your admins.</i>\n",
    "\n",
    "    <br/>\n",
    "    <br/>\n",
    "    Detailed error trying to access the endpoint:\n",
    "    <div class=\"code dbdemos_block\">{str(e)}</div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    if \"403\" in str(e):\n",
    "      display_error(f\"Permission error on Vector Search index {index_name} using the endpoint {vs_endpoint_name} and secret {secret_scope}/{secret_key}\", vs_error)\n",
    "    else:\n",
    "      display_error(f\"Unkown error accessing the Vector Search index {index_name} using the endpoint {vs_endpoint_name} and secret {secret_scope}/{secret_key}\", vs_error)\n",
    "  def get_wid():\n",
    "    try:\n",
    "      return dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('orgId')\n",
    "    except:\n",
    "      return None\n",
    "  if get_wid() in [\"5206439413157315\", \"984752964297111\", \"1444828305810485\", \"2556758628403379\"]:\n",
    "    print(f\"----------------------------\\nYou are in a Shared FE workspace. Please don't override the secret value (it's set to the SP `{secret_principal}`).\\n---------------------------\")\n",
    "\n",
    "  if not error:\n",
    "    print('Secret and permissions seems to be properly setup, you can continue the demo!')\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def pprint(obj):\n",
    "  import pprint\n",
    "  pprint.pprint(obj, compact=True, indent=1, width=100)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#Temp workaround to test if a table exists in shared cluster mode in DBR 14.2 (see SASP-2467)\n",
    "def table_exists(table_name):\n",
    "    try:\n",
    "        spark.table(table_name).isEmpty()\n",
    "    except:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f84a2519-6907-4a4b-b171-2b69b34bc7ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\nEndpoint named kb_vs_endpoint is ready.\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "import time\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = 'kb_vs_endpoint'\n",
    "\n",
    "# def endpoint_exists(client, endpoint_name):\n",
    "#     return endpoint_name in client.list_endpoints()\n",
    "\n",
    "def wait_for_endpoint_to_be_ready(client, endpoint_name, max_attempts=10, wait_time=5):\n",
    "    attempts = 0\n",
    "    while attempts < max_attempts:\n",
    "        if endpoint_exists(client, endpoint_name) and endpoint_status(client, endpoint_name) == \"RUNNING\":\n",
    "            print(f\"Endpoint {endpoint_name} is ready.\")\n",
    "            return\n",
    "        print(f\"Waiting for endpoint {endpoint_name} to be ready...\")\n",
    "        time.sleep(wait_time)\n",
    "        attempts += 1\n",
    "    print(f\"Endpoint {endpoint_name} did not become ready within the specified attempts and wait time.\")\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "\n",
    "# wait_for_vs_endpoint_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c4c997-9aa9-4229-9018-5d64a9cc2e22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index devpost1.default.kb_vector_index_db on table devpost1.default.kb_vector_db is ready\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "#The table we'd like to index\n",
    "source_table_fullname = f\"{catalog}.{db}.kb_vector_db\"\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{db}.kb_vector_index_db\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  vsc.create_delta_sync_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=vs_index_fullname,\n",
    "    source_table_name=source_table_fullname,\n",
    "    pipeline_type=\"TRIGGERED\", #Sync needs to be manually triggered\n",
    "    primary_key=\"id\",\n",
    "    embedding_dimension=1024, #Match your model embedding size (bge)\n",
    "    embedding_vector_column=\"embedding\"\n",
    "  )\n",
    "  #Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "else:\n",
    "  #Trigger a sync to update our vs content with the new data saved in the table\n",
    "  wait_for_index_to_be_ready(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "  vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).sync()\n",
    "\n",
    "print(f\"index {vs_index_fullname} on table {source_table_fullname} is ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cfdd1db-ffee-4207-9837-4fe608e656fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\nThe endpoint 'kb_vs_endpoint' is not ready yet. Current status: ONLINE\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "def check_endpoint_status(endpoint_name):\n",
    "    vsc = VectorSearchClient()\n",
    "    endpoints = vsc.list_endpoints()\n",
    "    \n",
    "    for endpoint in endpoints['endpoints']:\n",
    "        if endpoint['name'] == endpoint_name:\n",
    "            return endpoint['endpoint_status']['state']\n",
    "    \n",
    "    return \"Endpoint not found\"\n",
    "\n",
    "endpoint_name = VECTOR_SEARCH_ENDPOINT_NAME\n",
    "status = check_endpoint_status(VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "\n",
    "if status == \"RUNNING\":\n",
    "    print(f\"The endpoint '{endpoint_name}' is in a ready state.\")\n",
    "else:\n",
    "    print(f\"The endpoint '{endpoint_name}' is not ready yet. Current status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2af472-d768-4148-8de0-b10ab1579cae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf', 'Ansh Tiwari is a student at Arizona State University and is doing bachelors of science in computer science. He is a senior and has a GPA of 4.0.', 0.72141725]]\n"
     ]
    }
   ],
   "source": [
    "question = \"Where does Ansh Tiwari study ?\"\n",
    "\n",
    "response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [question]})\n",
    "embeddings = [e['embedding'] for e in response.data]\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_vector=embeddings[0],\n",
    "  columns=[\"url\", \"content\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d65215f9-f68f-4d1f-8f2c-d5ff67c9ad20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a6b3a3-a27a-4ce8-9581-0d4934ba9482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf',\n  'Ansh Tiwari is a student at Arizona State University and is doing bachelors of science in '\n  'computer science. He is a senior and has a GPA of 4.0.',\n  0.72141725]]\n"
     ]
    }
   ],
   "source": [
    "question = \"Where does Ansh Tiwari study ?\"\n",
    "\n",
    "response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [question]})\n",
    "embeddings = [e['embedding'] for e in response.data]\n",
    "\n",
    "results = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname).similarity_search(\n",
    "  query_vector=embeddings[0],\n",
    "  columns=[\"url\", \"content\"],\n",
    "  num_results=1)\n",
    "docs = results.get('result', {}).get('data_array', [])\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8190c74e-6260-4a24-90e5-c80b8ca2f219",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have real-time data access, so I can't provide the current location where Ansh studies.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables = [\"question\"],\n",
    "  template = \"You are an assistant. Give a short answer to this question: {question}\"\n",
    ")\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-dbrx-instruct\", max_tokens = 500)\n",
    "\n",
    "chain = (\n",
    "  prompt\n",
    "  | chat_model\n",
    "  | StrOutputParser()\n",
    ")\n",
    "print(chain.invoke({\"question\": \"Where does Ansh study ?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736dc72b-e866-47cc-93f4-df6157306347",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt_with_history_str = \"\"\"\n",
    "Your are a Documents chatbot. Please answer those questions only related to documents. If you don't know or not related, don't answer.\n",
    "\n",
    "Here is a history between you and a human: {chat_history}\n",
    "\n",
    "Now, please answer this question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_with_history = PromptTemplate(\n",
    "  input_variables = [\"chat_history\", \"question\"],\n",
    "  template = prompt_with_history_str\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fc8c56-dcc1-4925-b488-c813dbfaafc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry for any confusion, but I don't have information about what Ansh Tiwari studies. The previous conversation only mentioned where Ansh Tiwari studies, not what field of study he is in.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "#The question is the last entry of the history\n",
    "def extract_question(input):\n",
    "    return input[-1][\"content\"]\n",
    "\n",
    "#The history is everything before the last question\n",
    "def extract_history(input):\n",
    "    return input[:-1]\n",
    "\n",
    "chain_with_history = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n",
    "    }\n",
    "    | prompt_with_history\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain_with_history.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Where does Ansh Tiwari study\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"Ansh Tiwari study in Arizona State University.\"}, \n",
    "        {\"role\": \"user\", \"content\": \"What does he study ?\"}\n",
    "    ]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787f97b2-80c0-41d7-90cb-7949b217002a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "# Question Classifier - Relevant or not\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-dbrx-instruct\", max_tokens = 200)\n",
    "\n",
    "is_question_about_databricks_str = \"\"\"\n",
    "You are classifying documents to know if this question is related with content in the documents  or something from a very different field. Also answer no if the last part is inappropriate. \n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Question: Knowing this followup history: Where does Ansh Tiwari study?, classify this question: Do you have more details?\n",
    "Expected Response: Yes\n",
    "\n",
    "Question: Knowing this followup history: Where does Ansh Tiwari study?, classify this question: Write me a song.\n",
    "Expected Response: No\n",
    "\n",
    "Only answer with \"yes\" or \"no\". \n",
    "\n",
    "Knowing this followup history: {chat_history}, classify this question: {question}\n",
    "\"\"\"\n",
    "\n",
    "is_question_about_databricks_prompt = PromptTemplate(\n",
    "  input_variables= [\"chat_history\", \"question\"],\n",
    "  template = is_question_about_databricks_str\n",
    ")\n",
    "\n",
    "is_about_databricks_chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n",
    "    }\n",
    "    | is_question_about_databricks_prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#Returns \"Yes\" as this is about Databricks: \n",
    "print(is_about_databricks_chain.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Where does Ansh Tiwari study\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"He studies at Arizona State University.\"}, \n",
    "        {\"role\": \"user\", \"content\": \"What does he study?\"}\n",
    "    ]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd23a516-d594-47d0-a715-dd4ec6f42f09",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.\n"
     ]
    }
   ],
   "source": [
    "#Return \"no\" as this isn't about Databricks\n",
    "print(is_about_databricks_chain.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the meaning of life?\"}\n",
    "    ]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0993c0-3a3e-46e9-943d-b038f4d17bef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "  .dbdemos_install{\n",
       "                      font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji,FontAwesome;\n",
       "  color: #3b3b3b;\n",
       "  box-shadow: 0 .15rem 1.15rem 0 rgba(58,59,69,.15)!important;\n",
       "  padding: 10px 20px 20px 20px;\n",
       "  margin: 10px;\n",
       "  font-size: 14px !important;\n",
       "  }\n",
       "  .dbdemos_block{\n",
       "      display: block !important;\n",
       "      width: 900px;\n",
       "  }\n",
       "  .code {\n",
       "      padding: 5px;\n",
       "      border: 1px solid #e4e4e4;\n",
       "      font-family: monospace;\n",
       "      background-color: #f5f5f5;\n",
       "      margin: 5px 0px 0px 0px;\n",
       "      display: inline;\n",
       "  }\n",
       "  </style>\n",
       "      <div class=\"dbdemos_install\">\n",
       "                          <h1 style=\"color: #eb0707\">Configuration error: We couldn't access the Pat Token saved in the secret dbdemos/rag_sp_token</h1> \n",
       "                            \n",
       "    <strong>This likely means your secret isn't set or not accessible for your user</strong>.<br/><br/>\n",
       "    Your model will be served using Databrick Serverless endpoint and needs a Pat Token to authenticate.<br/>\n",
       "    <strong> This must be saved as a secret to be accessible when the model is deployed.</strong><br/><br/>\n",
       "    Here is how you can add the Pat Token as a secret available within your notebook and for the model:\n",
       "    <ul>\n",
       "    <li>\n",
       "      first, setup the Databricks CLI on your laptop or using this cluster terminal:\n",
       "      <div class=\"code dbdemos_block\">pip install databricks-cli</div>\n",
       "    </li>\n",
       "    <li> \n",
       "      Configure the CLI. You'll need your workspace URL and a PAT token from your profile page\n",
       "      <div class=\"code dbdemos_block\">databricks configure</div>\n",
       "    </li>  \n",
       "    <li>\n",
       "      Create the dbdemos scope:\n",
       "      <div class=\"code dbdemos_block\">databricks secrets create-scope dbdemos</div>\n",
       "    <li>\n",
       "      Save your service principal secret. It will be used by the Model Endpoint to autenticate. <br/>\n",
       "      If this is a demo/test, you can use one of your PAT token.\n",
       "      <div class=\"code dbdemos_block\">databricks secrets put-secret dbdemos rag_sp_token</div>\n",
       "    </li>\n",
       "    <li>\n",
       "      Optional - if someone else created the scope, make sure they give you read access to the secret:\n",
       "      <div class=\"code dbdemos_block\">databricks secrets put-acl dbdemos 'atiwar31@asu.edu' READ</div>\n",
       "\n",
       "    </li>  \n",
       "    </ul>  \n",
       "    <br/>\n",
       "    Detailed error trying to access the secret:\n",
       "      <div class=\"code dbdemos_block\">Secret does not exist with scope: dbdemos and key: rag_sp_token</div>\n",
       "                        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index_name=f\"{catalog}.{db}.kb_vector_index_db\"\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "\n",
    "#Let's make sure the secret is properly setup and can access our vector search index. Check the quick-start demo for more guidance\n",
    "test_demo_permissions(host, secret_scope=\"dbdemos\", secret_key=\"rag_sp_token\", vs_endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=index_name, embedding_endpoint_name=\"databricks-bge-large-en\", managed_embeddings = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66bcfa9a-b62a-4aae-8bef-1e6878011bb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from langchain_community.embeddings import DatabricksEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(\"dbdemos\", \"rag_sp_token\")\n",
    "\n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "\n",
    "def get_retriever(persist_dir: str = None):\n",
    "    # os.environ[\"DATABRICKS_HOST\"] = host\n",
    "    #Get the vector search index\n",
    "    # vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ[\"DATABRICKS_TOKEN\"])\n",
    "    vsc = VectorSearchClient()\n",
    "    vs_index = vsc.get_index(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "        index_name=index_name\n",
    "    )\n",
    "\n",
    "    # Create the retriever\n",
    "    vectorstore = DatabricksVectorSearch(\n",
    "        vs_index, text_column=\"content\", embedding=embedding_model, columns=[\"url\"]\n",
    "    )\n",
    "    return vectorstore.as_retriever(search_kwargs={'k': 4})\n",
    "\n",
    "retriever = get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a4e26fe-1587-423e-b79a-a7152f3611a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Ansh Tiwari is a student at Arizona State University and is doing bachelors of science in computer science. He is a senior and has a GPA of 4.0.', metadata={'url': 'dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf', 'id': 1.0})]\n"
     ]
    }
   ],
   "source": [
    "retrieve_document_chain = (\n",
    "    itemgetter(\"messages\") \n",
    "    | RunnableLambda(extract_question)\n",
    "    | retriever\n",
    ")\n",
    "print(retrieve_document_chain.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What does Ansh Tiwari study ?\"}]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3161909e-1573-42a0-9670-cceffd48552f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test retriever query without history: Where does Ansh Tiwari study\nTest retriever question, summarized with history: Query: \"What does Ansh Tiwari study at Arizona State University?\"\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "generate_query_to_retrieve_context_template = \"\"\"\n",
    "Based on the chat history below, we want you to generate a query for an external data source to retrieve relevant documents so that we can better answer the question. The query should be in natual language. The external data source uses similarity search to search for relevant documents in a vector space. So the query should be similar to the relevant documents semantically. Answer with only the query. Do not add explanation.\n",
    "\n",
    "Chat history: {chat_history}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "generate_query_to_retrieve_context_prompt = PromptTemplate(\n",
    "  input_variables= [\"chat_history\", \"question\"],\n",
    "  template = generate_query_to_retrieve_context_template\n",
    ")\n",
    "\n",
    "generate_query_to_retrieve_context_chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "        \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),\n",
    "    }\n",
    "    | RunnableBranch(  #Augment query only when there is a chat history\n",
    "      (lambda x: x[\"chat_history\"], generate_query_to_retrieve_context_prompt | chat_model | StrOutputParser()),\n",
    "      (lambda x: not x[\"chat_history\"], RunnableLambda(lambda x: x[\"question\"])),\n",
    "      RunnableLambda(lambda x: x[\"question\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "#Let's try it\n",
    "output = generate_query_to_retrieve_context_chain.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Where does Ansh Tiwari study\"}\n",
    "    ]\n",
    "})\n",
    "print(f\"Test retriever query without history: {output}\")\n",
    "\n",
    "output = generate_query_to_retrieve_context_chain.invoke({\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Where does Ansh Tiwari study ?\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"Ansh Tiwari study at Arizona State University.\"}, \n",
    "        {\"role\": \"user\", \"content\": \"What does he study?\"}\n",
    "    ]\n",
    "})\n",
    "print(f\"Test retriever question, summarized with history: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a64177be-606f-43cc-92cc-ed746dc6d54e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch, RunnableParallel, RunnablePassthrough\n",
    "\n",
    "question_with_history_and_context_str = \"\"\"\n",
    "You are a trustful assistant for Propriety Documents users. You are answering questions related to content in the documents. If you do not know the answer to a question, you truthfully say you do not know. Read the discussion to get the context of the previous conversation. In the chat discussion, you are referred to as \"system\". The user is referred to as \"user\".\n",
    "\n",
    "Discussion: {chat_history}\n",
    "\n",
    "Here's some context which might or might not help you answer: {context}\n",
    "\n",
    "Answer straight, do not repeat the question, do not start with something like: the answer to the question, do not add \"AI\" in front of your answer, do not say: here is the answer, do not mention the context or the question. \n",
    "Always Ensure that generated information is not sensitive. \n",
    "In other words, Sensitive content generation is strictly prohibited.\n",
    "\n",
    "\n",
    "Based on this history and context, answer this question: {question}\n",
    "\"\"\"\n",
    "\n",
    "question_with_history_and_context_prompt = PromptTemplate(\n",
    "  input_variables= [\"chat_history\", \"context\", \"question\"],\n",
    "  template = question_with_history_and_context_str\n",
    ")\n",
    "\n",
    "def format_context(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "def extract_source_urls(docs):\n",
    "    return [d.metadata[\"url\"] for d in docs]\n",
    "\n",
    "relevant_question_chain = (\n",
    "  RunnablePassthrough() |\n",
    "  {\n",
    "    \"relevant_docs\": generate_query_to_retrieve_context_prompt | chat_model | StrOutputParser() | retriever,\n",
    "    \"chat_history\": itemgetter(\"chat_history\"), \n",
    "    \"question\": itemgetter(\"question\")\n",
    "  }\n",
    "  |\n",
    "  {\n",
    "    \"context\": itemgetter(\"relevant_docs\") | RunnableLambda(format_context),\n",
    "    \"sources\": itemgetter(\"relevant_docs\") | RunnableLambda(extract_source_urls),\n",
    "    \"chat_history\": itemgetter(\"chat_history\"), \n",
    "    \"question\": itemgetter(\"question\")\n",
    "  }\n",
    "  |\n",
    "  {\n",
    "    \"prompt\": question_with_history_and_context_prompt,\n",
    "    \"sources\": itemgetter(\"sources\")\n",
    "  }\n",
    "  |\n",
    "  {\n",
    "    \"result\": itemgetter(\"prompt\") | chat_model | StrOutputParser(),\n",
    "    \"sources\": itemgetter(\"sources\")\n",
    "  }\n",
    ")\n",
    "\n",
    "irrelevant_question_chain = (\n",
    "  RunnableLambda(lambda x: {\"result\": 'I cannot answer questions that are not in the documents.', \"sources\": []})\n",
    ")\n",
    "\n",
    "branch_node = RunnableBranch(\n",
    "  (lambda x: \"yes\" in x[\"question_is_relevant\"].lower(), relevant_question_chain),\n",
    "  (lambda x: \"no\" in x[\"question_is_relevant\"].lower(), irrelevant_question_chain),\n",
    "  irrelevant_question_chain\n",
    ")\n",
    "\n",
    "full_chain = (\n",
    "  {\n",
    "    \"question_is_relevant\": is_about_databricks_chain,\n",
    "    \"question\": itemgetter(\"messages\") | RunnableLambda(extract_question),\n",
    "    \"chat_history\": itemgetter(\"messages\") | RunnableLambda(extract_history),    \n",
    "  }\n",
    "  | branch_node\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "405f872a-7434-4b0c-8ad9-9bce267c804a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with a non relevant question...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
       "        Where does Ansh Tiwari study?\n",
       "      </div>\n",
       "      <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
       "        <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
       "        Ansh Tiwari study at Arizona State University.\n",
       "      </div>\n",
       "      <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
       "        Why is the sky blue?\n",
       "      </div>\n",
       "      <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
       "        <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
       "        I cannot answer questions that are not in the documents.\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "non_relevant_dialog = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Where does Ansh Tiwari study?\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"Ansh Tiwari study at Arizona State University.\"}, \n",
    "        {\"role\": \"user\", \"content\": \"Why is the sky blue?\"}\n",
    "    ]\n",
    "}\n",
    "print(f'Testing with a non relevant question...')\n",
    "response = full_chain.invoke(non_relevant_dialog)\n",
    "display_chat(non_relevant_dialog[\"messages\"], response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87cb1127-08c2-4395-a8b5-c40d492d8085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with relevant history and question...\n{'result': 'Computer science at Arizona State University.', 'sources': ['dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf']}\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
       "        Where does Ansh Tiwari study?\n",
       "      </div>\n",
       "      <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
       "        <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
       "        Ansh Tiwari study at Arizona State University.\n",
       "      </div>\n",
       "      <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
       "        What does he study?\n",
       "      </div>\n",
       "      <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
       "        <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
       "        Computer science at Arizona State University.<br/><br/><br/><strong>Sources:</strong><br/> <ul><li><a href=\"dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf\">dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf</a></li></ul>\n",
       "      </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dialog = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Where does Ansh Tiwari study?\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"Ansh Tiwari study at Arizona State University.\"}, \n",
    "        {\"role\": \"user\", \"content\": \"What does he study?\"}\n",
    "    ]\n",
    "}\n",
    "print(f'Testing with relevant history and question...')\n",
    "response = full_chain.invoke(dialog)\n",
    "print(response)\n",
    "display_chat(dialog[\"messages\"], response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6394018-67cb-4bee-87e6-04a6656f6894",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/07 22:42:43 INFO mlflow.types.utils: MLflow 2.9.0 introduces model signature with new data types for lists and dictionaries. For input such as Dict[str, Union[scalars, List, Dict]], we infer dictionary values types as `List -> Array` and `Dict -> Object`. \n2024/05/07 22:42:43 INFO mlflow.types.utils: MLflow 2.9.0 introduces model signature with new data types for lists and dictionaries. For input such as Dict[str, Union[scalars, List, Dict]], we infer dictionary values types as `List -> Array` and `Dict -> Object`. \n/databricks/python/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4c51eaa4ca47b3aade00a30934f0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading artifacts:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/07 22:42:45 INFO mlflow.store.artifact.cloud_artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\nSuccessfully registered model 'devpost1.default.kb_rag_llm_chatbot_model'.\nCreated version '1' of model 'devpost1.default.kb_rag_llm_chatbot_model'.\n"
     ]
    }
   ],
   "source": [
    "import cloudpickle\n",
    "import langchain\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.set_registry_uri(\"kb_rag_llm-uc\")\n",
    "model_name = f\"{catalog}.{db}.kb_rag_llm_chatbot_model\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"kb_chatbot_rag\") as run:\n",
    "    #Get our model signature from input/output\n",
    "    output = full_chain.invoke(dialog)\n",
    "    signature = infer_signature(dialog, output)\n",
    "\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        full_chain,\n",
    "        loader_fn=get_retriever,  # Load the retriever with DATABRICKS_TOKEN env as secret (for authentication).\n",
    "        artifact_path=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        pip_requirements=[\n",
    "            \"mlflow==\" + mlflow.__version__,\n",
    "            \"langchain==\" + langchain.__version__,\n",
    "            \"databricks-vectorsearch\",\n",
    "            \"pydantic==2.5.2 --no-binary pydantic\",\n",
    "            \"cloudpickle==\"+ cloudpickle.__version__\n",
    "        ],\n",
    "        input_example=dialog,\n",
    "        signature=signature,\n",
    "        example_no_conversion=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec9719e-8db7-4abd-92a4-92ac9bf2461f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e196240f7d1941ea938c1ab1ea1ae156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/07 22:43:36 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'result': 'Computer science at Arizona State University.',\n",
       " 'sources': ['dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf']}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mlflow.langchain.load_model(model_info.model_uri)\n",
    "model.invoke(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca0d7c3b-630a-4673-a3c8-c1f1a919c86c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'result': '4.0',\n",
       " 'sources': ['dbfs:/Volumes/devpost1/default/kb_dir/sample_pdf.pdf']}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialog = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Where does Ansh Tiwari study?\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"Ansh Tiwari study at Arizona State University.\"}, \n",
    "        {\"role\": \"user\", \"content\": \"What does he study?\"},\n",
    "        {\"role\": \"assistant\", \"content\" : \"Computer science at Arizona State University.\"},\n",
    "        {\"role\": \"user\", \"content\": \"what is his cgpa?\"},\n",
    "    ]\n",
    "\n",
    "}\n",
    "model.invoke(dialog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc8d84a5-a631-4ec2-8538-bbae42564131",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_registry_uri('kb_rag_llm-uc')\n",
    "client = MlflowClient()\n",
    "model_name = f\"{catalog}.{db}.kb_rag_llm_chatbot_model\"\n",
    "serving_endpoint_name = f\"dbdemos_endpoint_advanced_{catalog}_{db}\"[:63]\n",
    "latest_model = client.get_model_version_by_alias(model_name, \"prod\")\n",
    "\n",
    "#TODO: use the sdk once model serving is available.\n",
    "serving_client = EndpointApiClient()\n",
    "# Start the endpoint using the REST API (you can do it using the UI directly)\n",
    "auto_capture_config = {\n",
    "    \"catalog_name\": catalog,\n",
    "    \"schema_name\": db,\n",
    "    \"table_name_prefix\": serving_endpoint_name\n",
    "    }\n",
    "environment_vars={\"DATABRICKS_TOKEN\": \"{{secrets/dbdemos/rag_sp_token}}\"}\n",
    "serving_client.create_endpoint_if_not_exists(serving_endpoint_name, model_name=model_name, model_version = latest_model.version, workload_size=\"Small\", scale_to_zero_enabled=True, wait_start = True, auto_capture_config=auto_capture_config, environment_vars=environment_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7e12400-9abe-46cc-8ae4-6d8a676e6ec6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create or update serving endpoint\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, ServedModelInput, ServedModelInputWorkloadSize\n",
    "\n",
    "serving_endpoint_name = f\"dbdemos_endpoint_{catalog}_{db}\"[:63]\n",
    "latest_model_version = get_latest_model_version(model_name)\n",
    "\n",
    "w = WorkspaceClient()\n",
    "endpoint_config = EndpointCoreConfigInput(\n",
    "    name=serving_endpoint_name,\n",
    "    served_models=[\n",
    "        ServedModelInput(\n",
    "            model_name=model_name,\n",
    "            model_version=latest_model_version,\n",
    "            workload_size=ServedModelInputWorkloadSize.SMALL,\n",
    "            scale_to_zero_enabled=True,\n",
    "            environment_vars={\n",
    "                \"DATABRICKS_TOKEN\": \"{{secrets/dbdemos/rag_sp_token}}\",  # <scope>/<secret> that contains an access token\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "existing_endpoint = next(\n",
    "    (e for e in w.serving_endpoints.list() if e.name == serving_endpoint_name), None\n",
    ")\n",
    "serving_endpoint_url = f\"{host}/ml/endpoints/{serving_endpoint_name}\"\n",
    "if existing_endpoint == None:\n",
    "    print(f\"Creating the endpoint {serving_endpoint_url}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.create_and_wait(name=serving_endpoint_name, config=endpoint_config)\n",
    "else:\n",
    "    print(f\"Updating the endpoint {serving_endpoint_url} to version {latest_model_version}, this will take a few minutes to package and deploy the endpoint...\")\n",
    "    w.serving_endpoints.update_config_and_wait(served_models=endpoint_config.served_models, name=serving_endpoint_name)\n",
    "    \n",
    "displayHTML(f'Your Model Endpoint Serving is now available. Open the <a href=\"/ml/endpoints/{serving_endpoint_name}\">Model Serving Endpoint page</a> for more details.')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3570846870649496,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Sample RAG Vector DB",
   "widgets": {
    "reset_all_data": {
     "currentValue": "false",
     "nuid": "edbd8627-33e1-4808-b440-ca150d55db61",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": "Reset Data",
      "name": "reset_all_data",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "use_old_langchain": {
     "currentValue": "false",
     "nuid": "85d02121-665e-4465-b018-ab728d72861b",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": "Use packages in init script or skip",
      "name": "use_old_langchain",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
